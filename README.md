# model-training

<!-- PYLINT_BADGE_START -->
![Pylint Score](https://img.shields.io/badge/pylint-10%2E00%2F10-brightgreen)
<!-- PYLINT_BADGE_END -->

<!-- COVERAGE_BADGE_START -->
![Coverage](https://codecov.io/github/remla25-team21/model-training/branch/feat%2Fa4-ml-testing/graph/badge.svg?token=L9ICV9K86O)
<!-- COVERAGE_BADGE_END -->

<!-- ML_SCORE_BADGE_START -->
![ML Test Score](https://img.shields.io/badge/ML%20Test%20Score-12%2F12-brightgreen)
<!-- ML_SCORE_BADGE_END -->

This repository contains the training pipeline for the sentiment analysis model used in our REMLA project. 

- It uses the [lib-ml](https://github.com/remla25-team21/lib-ml) library for data preprocessing and saves the trained model (`sentiment_model_*.pkl`) as a release artifact.
- The training dataset can be found in `data/raw/a1_RestaurantReviews_HistoricDump.tsv`.
- The project now uses [DVC](https://dvc.org/) (Data Version Control) to track data, models, and metrics.

## Quick Start (TL;DR)

> [!NOTE]
> Follow these steps to get the project up and running quickly:

1.  **Clone the repository**
    ```bash
    git clone https://github.com/remla25-team21/model-training.git
    cd model-training
    ```

2.  **Install the required dependencies**
    ```bash
    pip install -r requirements.txt
    ```

3.  **(Optional) Configure DVC remote storage**
    This is only needed if you want to push changes to the remote storage or if `dvc pull` fails without authentication. See [DVC Configuration](#configuration) for details.
    ```bash
    # Example:
    # dvc remote modify storage --local gdrive_use_service_account true
    # dvc remote modify storage --local gdrive_service_account_json_file_path <path/to/your/service_account.json>
    ```

4.  **Pull the data from remote storage**
    If you encounter issues, see the [Troubleshooting DVC Pull](#troubleshooting-dvc-pull) section.
    ```bash
    dvc pull
    ```

5.  **Run the DVC pipeline**
    This will preprocess data, train the model, and evaluate it.
    ```bash
    dvc repro
    ```

6.  **Run tests**
    ```bash
    pytest
    ```

7.  **Generate coverage report**
    ```bash
    coverage run -m pytest
    coverage report # Prints summary in terminal 
    coverage xml    # Generates coverage.xml file in the root directory
    ```

## DVC Pipeline Details

The training process is managed by a DVC pipeline, divided into three main stages:

1.  **Preprocessing**: Data preparation and feature extraction.
2.  **Training**: Model training with hyperparameter tuning.
3.  **Evaluation**: Model evaluation and metrics generation.

### Configuration

To use DVC with remote storage (e.g., Google Drive), you might need to configure it. This typically involves setting up credentials for DVC to access the remote storage.

```bash
dvc remote modify storage --local gdrive_use_service_account true
dvc remote modify storage --local gdrive_service_account_json_file_path <path/to/your/service_account.json>  # Replace with your Google Drive service account JSON file path
```
Ensure your service account has access to the shared Google Drive folder/files.

### Running the Pipeline

-   **Pull data and models from remote storage**:
    ```bash
    dvc pull
    ```
    This command downloads data, models, and other artifacts tracked by DVC from the configured remote storage.

-   **Run the complete pipeline**:
    ```bash
    dvc repro
    ```
    This command reproduces the pipeline, running stages whose dependencies have changed.

-   **Run a specific stage**:
    ```bash
    dvc repro <stage_name>  # e.g., dvc repro preprocess
    ```

### Inspecting Results

-   **View metrics**:
    ```bash
    dvc metrics show
    ```
    This displays metrics generated by the pipeline (e.g., model performance).

-   **View all experiments**:
    ```bash
    dvc exp show
    ```
    This command shows a table of all experiments, including their parameters and metrics.

### Troubleshooting DVC Pull

If you encounter issues with `dvc pull` (e.g., you don't have access to the Google Drive file or prefer not to set up DVC remote), you can download the primary dataset directly:

-   **Linux/macOS**:
    ```bash
    mkdir -p ./data/raw
    wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1mrWUgJlRCf_n_TbxPuuthJ9YsTBwGuRh' -O ./data/raw/a1_RestaurantReviews_HistoricDump.tsv
    ```

-   **Windows (PowerShell)**:
    ```powershell
    New-Item -ItemType Directory -Force -Path "./data/raw"
    Invoke-WebRequest -Uri "https://drive.google.com/uc?export=download&id=1mrWUgJlRCf_n_TbxPuuthJ9YsTBwGuRh" -OutFile "./data/raw/a1_RestaurantReviews_HistoricDump.tsv"
    ```

After downloading the dataset manually, you can proceed with the DVC pipeline:
```bash
dvc repro
```
DVC will recognize the local data file if its path and content match what's expected by the `preprocess` stage.

### Collaboration with DVC

For more details on collaborating with DVC, including branching and sharing experiments, refer to [./docs/dvc-ref.md](./docs/dvc-ref.md).

## Manual Training (Alternative to DVC)

If you prefer to run each stage of the training process manually without using `dvc repro`:

1.  **Install dependencies** (if not already done):
    ```bash
    pip install -r requirements.txt
    ```
2.  **Ensure data is available**:
    Download `a1_RestaurantReviews_HistoricDump.tsv` into `data/raw/` as described in the [Troubleshooting DVC Pull](#troubleshooting-dvc-pull) section.

3.  **Run the scripts sequentially**:
    ```bash
    # Preprocessing
    python src/preprocess.py

    # Training
    python src/train.py

    # Evaluation
    python src/evaluate.py
    ```

## Pipeline Outputs

The DVC pipeline and manual training scripts produce the following artifacts:

*   `data/processed/preprocessed_data_*.pkl`: Preprocessed data (features and labels).
*   `models/c1_BoW_Sentiment_Model_*.pkl`: Text vectorizer model (CountVectorizer).
*   `models/trained_model_*.pkl`: Trained ML model before final evaluation.
*   `models/sentiment_model_*.pkl`: Final ML model after evaluation (this is the model intended for release).
*   `metrics/metrics_*.json`: Model performance metrics.

(Note: File paths and naming conventions might include versioning or timestamps, indicated by `*`).

## Code Quality

We use several tools to maintain code quality, enforce standards, and identify potential issues.

### Linters

Linters help improve code quality by identifying errors, enforcing style rules, and spotting security issues without running the code.

**Linters Used:**

*   **Pylint**: Checks for coding errors and enforces coding standards.
*   **Flake8**: Checks code style (PEP8) and complexity.
*   **Bandit**: Scans for common security vulnerabilities in Python code.

**How to Run Linters:**

To run all linters and generate reports:

-   **For Mac/Linux**:
    ```bash
    bash lint.sh
    ```

-   **For Windows**:
    Use Git Bash or a similar shell that can execute bash scripts:
    1.  Ensure the script is executable:
        ```bash
        chmod +x lint.sh
        ```
    2.  Run the script:
        ```bash
        ./lint.sh
        ```

### ML Test Score

The quality and correctness of the ML components are assessed through a series of tests.

<!-- ML_TEST_SCORE_START -->
<!-- ML_TEST_SCORE_END -->
<!-- ML_TEST_SCORE_START -->
| Category              | Test Count | Automated? |
|-----------------------|------------|------------|
| Feature & Data         | ✅ 5        | ✅         |
| Model Development      | ✅ 5        | ✅         |
| ML Infrastructure      | ✅ 2        | ✅         |
| Monitoring             | ✅ 2        | ✅         |
| Mutamorphic Testing    | ✅ 3        | ✅         |
| Preprocessing Module   | ✅ 2        | ✅         |
| Training Module        | ✅ 5        | ✅         |
| Evaluation Module      | ✅ 4        | ✅         |

**Final Score:** 12/12
<!-- ML_TEST_SCORE_END -->